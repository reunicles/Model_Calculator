# Mathematical Foundations

> **Comprehensive mathematical foundations for transformer memory and compute analysis**

[![Transformer Calculator](https://img.shields.io/badge/Transformer-Calculator-blue.svg)](https://github.com/reunicles/Model_Calculator)
[![Mathematical Accuracy](https://img.shields.io/badge/Mathematical-Accuracy-green.svg)](https://github.com/reunicles/Model_Calculator)
[![MoE Support](https://img.shields.io/badge/MoE-Supported-orange.svg)](https://github.com/reunicles/Model_Calculator)

## ðŸŽ¯ Quick Reference

### Core Symbols
| Symbol | Meaning | Example |
|--------|---------|---------|
| **B** | Batch size | 1, 32, 128 |
| **S** | Sequence length | 2048, 8192, 65536 |
| **L** | Number of layers | 32, 80, 94 |
| **H** | Attention heads | 32, 64, 128 |
| **d** | Hidden dimension | 4096, 8192 |
| **d_h** | Head dimension (d/H) | 128, 64 |
| **d_ff** | MLP dimension | 4Ã—d, 8Ã—d |

### Master Equations
```math
Prefill FLOPs = L Ã— [5BSdÂ² + 2BSÂ²d]
Decode FLOPs  = L Ã— [5BdÂ² + 2BdS]
KV Cache     = B Ã— S Ã— H Ã— (d_k + d_v) Ã— bytes
```

---

## ðŸ“Š Per-Layer Operations

### Standard Transformer Layer
| Operation | FLOPs | Memory | Arithmetic Intensity |
|-----------|-------|--------|---------------------|
| **QKV Projections** | `3BSdÂ²` | `3BSd Ã— bytes` | `d/bytes` |
| **Attention Scores** | `2BSHd_hS` | `2BSHd_hS Ã— bytes` | `1/bytes` |
| **Attention Ã— V** | `2BSHd_hS` | `2BSHd_hS Ã— bytes` | `1/bytes` |
| **Output Projection** | `BSdÂ²` | `BSd Ã— bytes` | `d/bytes` |
| **MLP Up** | `BSdd_ff` | `BSdd_ff Ã— bytes` | `d_ff/bytes` |
| **MLP Down** | `BSdd_ff` | `BSdd_ff Ã— bytes` | `d_ff/bytes` |

**Total per layer**: Sum all operations  
**Total for model**: Multiply by L layers

---

## ðŸ”„ Operation Modes

### Prefill (Parallel Processing)
- **Memory**: Peak usage during sequence processing
- **FLOPS**: `L Ã— [5BSdÂ² + 2BSÂ²d]`
- **Attention**: O(SÂ²) - quadratic scaling
- **Flash Attention**: O(S) - linear scaling

### Decode (Autoregressive)
- **Memory**: Peak usage during generation
- **FLOPS**: `L Ã— [5BdÂ² + 2BdS]` per token
- **Attention**: O(S) - linear scaling
- **Flash Decode**: O(1) - constant scaling

### Pretraining (Full Training)
- **Memory**: Forward + Backward + Gradients + Optimizer
- **FLOPS**: 3Ã— forward pass (forward + backward)
- **Gradients**: FP32 parameter gradients
- **Optimizer**: Adam states (momentum + variance)

---

## ðŸ§  Memory Components

### 1. Attention Memory
```math
Standard: SÂ² Ã— B Ã— H Ã— bytes
Flash:    block_size Ã— B Ã— H Ã— bytes Ã— factor
```

### 2. MLP Memory
```math
Dense: 2BSdd_ff Ã— bytes
MoE:   K Ã— util Ã— 2BSd Ã— d_ff_moe Ã— bytes
```

### 3. KV Cache
```math
KV_bytes = B Ã— S Ã— H Ã— (d_k + d_v) Ã— bytes_kv
Read/Write = 2 Ã— KV_bytes per token
```

### 4. Expert Weights (MoE)
```math
Expert_weights = E Ã— 2 Ã— d Ã— d_ff_moe Ã— bytes
# Shared across all layers (no L multiplication)
```

---

## âš¡ Flash Attention Optimization

### Memory Reduction
| Sequence Length | Standard Memory | Flash Memory | Reduction |
|----------------|----------------|--------------|-----------|
| 1K | O(SÂ²) | O(S) | ~1000Ã— |
| 4K | O(SÂ²) | O(S) | ~4000Ã— |
| 16K | O(SÂ²) | O(S) | ~16000Ã— |
| 65K | O(SÂ²) | O(S) | ~65000Ã— |

### Sequence Length Bucketing
```math
factor = {
    1.1  if S â‰¤ 1K
    1.2  if S â‰¤ 4K  
    1.5  if S â‰¤ 16K
    2.0  if S â‰¤ 65K
    2.5  if S > 65K
}
```

---

## ðŸ”€ MoE (Mixture of Experts)

### Expert Utilization
```math
util = K / E
# K = top_k experts per token
# E = total number of experts
```

### MoE Memory Optimization
```math
HBM_Storage = E Ã— 2 Ã— d Ã— d_ff_moe Ã— bytes
GPU_Memory = K Ã— util Ã— 2 Ã— d Ã— d_ff_moe Ã— bytes
```

### Capacity Factor
```math
capacity = S Ã— B Ã— K Ã— capacity_factor
# Handles dropped tokens vs padding overhead
```

---

## ðŸ“ˆ Arithmetic Intensity Analysis

### Performance Ranges
| Mode | Attention | MLP | Total |
|------|-----------|-----|-------|
| **Pretraining** | 1K-1M FLOPS/B | 100-10K FLOPS/B | 10K-1M FLOPS/B |
| **Prefill** | 1K-1M FLOPS/B | 100-10K FLOPS/B | 10K-1M FLOPS/B |
| **Decode** | 0.1-10 FLOPS/B | 1-100 FLOPS/B | 1-100 FLOPS/B |

### Efficiency Classification
- **High**: >100 FLOPS/B (compute-bound)
- **Medium**: 10-100 FLOPS/B (balanced)
- **Low**: <10 FLOPS/B (memory-bound)

---

## ðŸŽ¯ Data Types

### Precision Support
| Type | Bytes | Usage | Memory Impact |
|------|-------|-------|---------------|
| **FP32** | 4 | Full precision | 4Ã— baseline |
| **FP16/BF16** | 2 | Standard | 2Ã— baseline |
| **INT8** | 1 | Quantized | 1Ã— baseline |
| **INT4** | 0.5 | Extreme quantization | 0.5Ã— baseline |

### KV Cache Optimization
```math
KV_FP16 = B Ã— S Ã— H Ã— (d_k + d_v) Ã— 2
KV_INT8 = B Ã— S Ã— H Ã— (d_k + d_v) Ã— 1
# 2Ã— memory reduction with INT8
```

---

## ðŸš€ Real-World Examples

### Model Scaling
| Parameters | Layers | Hidden | Heads | Typical S | Memory (GB) |
|-----------|--------|--------|-------|-----------|-------------|
| **7B** | 32 | 4096 | 32 | 2K-8K | 14-56 |
| **13B** | 40 | 5120 | 40 | 2K-8K | 26-104 |
| **70B** | 80 | 8192 | 64 | 2K-32K | 140-1120 |
| **MoE-8Ã—7B** | 32 | 4096 | 32 | 2K-8K | 56-224 |

### Flash Attention Impact
```math
# Example: 65K context length
Standard_Memory = 65KÂ² Ã— B Ã— H Ã— bytes â‰ˆ 4.2B Ã— B Ã— H Ã— bytes
Flash_Memory = 4096 Ã— B Ã— H Ã— bytes Ã— 2.5 â‰ˆ 10K Ã— B Ã— H Ã— bytes
Reduction = 420,000Ã—
```

---

## ðŸ”§ Implementation Notes

### Critical Considerations
1. **Attention Scaling**: O(SÂ²) â†’ O(S) with Flash Attention
2. **MoE Expert Sharing**: Weights shared across layers
3. **Memory Traffic**: Use GPU memory for MoE arithmetic intensity
4. **KV Cache**: Full sequence for storage, 1 token for computation
5. **Flash Decode**: O(S) â†’ O(1) for inference optimization

### Validation Ranges
- **Memory**: 1MB - 10TB (realistic model sizes)
- **FLOPS**: 1M - 1E FLOPS (from 1B to 1T parameters)
- **Arithmetic Intensity**: 0.1 - 1M FLOPS/B (memory to compute bound)

---

## ðŸ“š Further Reading

- [Transformer Calculator Documentation](../README.md)
- [CLI Usage Guide](../README.md#-cli-usage)
- [Web Interface Guide](../README.md#-enhanced-web-interface)
- [MoE Advanced Features](../README.md#-advanced-moe-optimizations)

---

*This document provides the mathematical foundations for accurate transformer model analysis. For implementation details and usage examples, see the main [README.md](../README.md).*